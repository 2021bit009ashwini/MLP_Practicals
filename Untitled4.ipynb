{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e277cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (m): 0.42155649706923126\n",
      "Intercept (b): 14.245236399085083\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = [5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]\n",
    "Y = [17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]\n",
    "\n",
    "# Number of data points\n",
    "N = len(X)\n",
    "\n",
    "# Calculate sums\n",
    "sum_x = sum(X)\n",
    "sum_y = sum(Y)\n",
    "sum_xy = sum(x * y for x, y in zip(X, Y))\n",
    "sum_x2 = sum(x ** 2 for x in X)\n",
    "\n",
    "# Calculate slope (m) and intercept (b)\n",
    "m = (N * sum_xy - sum_x * sum_y) / (N * sum_x2 - sum_x ** 2)\n",
    "b = (sum_y * sum_x2 - sum_x * sum_xy) / (N * sum_x2 - sum_x ** 2)\n",
    "\n",
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Intercept (b): {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbce985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9183016607031469\n",
      "Epoch 100, Loss: 0.891191719157456\n",
      "Epoch 200, Loss: 0.8907149109530661\n",
      "Epoch 300, Loss: 0.8907065248758261\n",
      "Epoch 400, Loss: 0.8907063773819474\n",
      "Epoch 500, Loss: 0.8907063747878331\n",
      "Epoch 600, Loss: 0.8907063747422079\n",
      "Epoch 700, Loss: 0.8907063747414053\n",
      "Epoch 800, Loss: 0.8907063747413912\n",
      "Epoch 900, Loss: 0.8907063747413911\n",
      "Weight W: [[0.33059586]]\n",
      "Bias b: [[-1.020517e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]).reshape(-1, 1)\n",
    "Y = np.array([17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "Y = (Y - np.mean(Y)) / np.std(Y)\n",
    "\n",
    "# Neural Network parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(input_size, output_size)\n",
    "b = np.zeros((1, output_size))\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X):\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "# Mean squared error loss\n",
    "def compute_loss(Y, Y_pred):\n",
    "    return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "# Backward propagation\n",
    "def backward(X, Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    dW = -2 * np.dot(X.T, (Y - Y_pred)) / m\n",
    "    db = -2 * np.sum(Y - Y_pred) / m\n",
    "    return dW, db\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Y_pred = forward(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(Y, Y_pred)\n",
    "    \n",
    "    # Backward pass\n",
    "    dW, db = backward(X, Y, Y_pred)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Results\n",
    "print(\"Weight W:\", W)\n",
    "print(\"Bias b:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a86ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 93.06719284724281\n",
      "Epoch 100, Loss: 0.77477924891854\n",
      "Epoch 200, Loss: 0.6986201469078495\n",
      "Epoch 300, Loss: 0.61567355879671\n",
      "Epoch 400, Loss: 0.5310112213929565\n",
      "Epoch 500, Loss: 0.44948443303136953\n",
      "Epoch 600, Loss: 0.3856491845664447\n",
      "Epoch 700, Loss: 0.3584258472760138\n",
      "Epoch 800, Loss: 0.34761536613110333\n",
      "Epoch 900, Loss: 0.3447859089732293\n",
      "Weight W1: [[ 0.70812921 -0.2477796  -0.02124272  1.5482106  -0.18764849 -0.22809432\n",
      "   1.12648154  0.79368996 -0.57397788  0.76859532]]\n",
      "Bias b1: [[-0.12553374  0.16548536 -0.54955813 -0.0202135   0.08582663  0.17980407\n",
      "  -0.37969619 -0.09720338  0.09703399  0.39425476]]\n",
      "Weight W2: [[-0.47364018 -0.60015013  0.2627328  -1.91328024 -1.72491783 -0.56227625\n",
      "  -1.01300105  0.36537861 -0.93083486 -1.4123037 ]\n",
      " [ 1.46452771 -0.22931201  0.06752811 -1.42474819 -0.54573858  0.09044887\n",
      "  -1.17883973  0.40774098 -0.58431773 -0.29169375]\n",
      " [-0.61503619  1.67700124  0.00700571 -1.05771093  0.82254491 -1.22084365\n",
      "   0.20813427 -1.9003801  -1.32818605  0.19686124]\n",
      " [ 0.70712229 -0.24079281 -0.05447873 -0.3011037  -1.47852199 -0.71992312\n",
      "  -0.45511411  1.20650355  0.28427344 -1.76304016]\n",
      " [ 0.32444733 -0.38374743 -0.676922    0.61167629  1.02343851  0.91678736\n",
      "  -0.85897808 -0.28607841  0.34368767  0.97554513]\n",
      " [-0.48028864 -0.18878335 -1.10668487 -1.19620662  0.80870895  1.33300366\n",
      "  -0.10390384  1.04031372  0.37931009 -0.64511975]\n",
      " [ 0.32889506  1.11067129  0.11539711  1.56464366 -2.6197451   0.8219025\n",
      "  -0.01630814 -0.16450646  0.0652711  -1.98756891]\n",
      " [-0.23546586  0.14943002  1.52875227 -0.51827022 -0.8084936  -0.50176706\n",
      "   0.89547863  0.40038957 -0.55540507  0.51326743]\n",
      " [ 0.10148567  0.97387584 -0.70205309 -0.32766215 -0.41304938 -1.49161514\n",
      "   0.25564227  0.3078369   0.03184585 -0.23458713]\n",
      " [-1.43228698 -0.56747248 -0.38101969 -0.80227727 -0.14237644  0.39032378\n",
      "   1.94235535  0.25967056  0.21836394 -0.07444592]]\n",
      "Bias b2: [[-0.07463316 -0.31453986 -0.20723261  0.          0.05326938 -0.14122967\n",
      "   0.04249393  0.34224678  0.06580238  0.        ]]\n",
      "Weight W3: [[-1.92243221 -0.02651388  0.04517451  2.46470665 -0.19028452  0.30829345\n",
      "  -0.04816669 -1.16867321  1.13837916  0.75193303]\n",
      " [ 0.71532675 -0.90938745  1.00163226 -1.41025438  0.64129028  2.01508304\n",
      "  -0.99247173 -0.56629773 -0.01875061 -0.50347565]\n",
      " [-1.57087304  0.06856297 -1.15186727  0.42207983 -0.90714294  1.54870823\n",
      "  -0.80927159 -0.38472866  0.78708276 -1.23385212]\n",
      " [ 0.22745993  1.30714275 -1.60748323  0.18463386  0.25988279  0.78182287\n",
      "  -1.23695071 -1.32045661  0.52194157  0.29698467]\n",
      " [ 0.26430066  0.34644821 -0.68002472  0.21927548  0.29350076 -0.70037745\n",
      "   1.87186669  0.46022667 -1.1913035   0.65655361]\n",
      " [-0.97443228  0.7870846   1.15859558 -0.82079607  0.96339715  0.41306702\n",
      "   0.82232573  1.89631268 -0.24538812 -0.75370843]\n",
      " [-0.92099872 -0.81581028 -0.16309078  0.43558634  0.28628364  0.72093312\n",
      "   0.05754399  1.55442728 -0.29003629  2.71037351]\n",
      " [ 0.68279792 -0.85715756 -1.11114658  0.47322854 -0.21898379  0.72586605\n",
      "   0.57236646 -0.08496774 -0.85867461 -1.53131021]\n",
      " [-0.42984608  0.85639879  0.21409374 -1.26347583  0.17346127  0.39852839\n",
      "  -0.872176    0.13599636  0.05820872 -1.14328844]\n",
      " [ 0.35778736  0.56078453  1.08305124  1.05380205 -1.37766937 -0.93782504\n",
      "   0.51503527  0.51378595  0.51504769  3.85273149]]\n",
      "Bias b3: [[ 0.16629528  0.         -0.0889756  -0.05184322  0.00845018 -0.00042841\n",
      "   0.24275021 -0.02011887 -0.02626093 -0.01132564]]\n",
      "Weight W4: [[ 0.67764307  1.06567392  0.93632064  0.62963969 -0.398309    0.75813235\n",
      "  -0.80973114 -0.23681861 -0.50677368  0.26923829]\n",
      " [ 2.31465857 -1.86726519  0.68626019 -1.61271587 -0.47193187  1.0889506\n",
      "   0.06428002 -1.07774478 -0.71530371  0.67959775]\n",
      " [-0.73036663 -0.081419    0.04557184 -0.65160035  1.79002975  0.63391902\n",
      "  -2.21001953  0.18645431 -0.66178646  0.88771502]\n",
      " [-0.78777397 -0.11473644  0.49919909  0.84567352 -1.20029641 -0.33479337\n",
      "  -0.47981458 -0.65332923  1.80322131  0.40311989]\n",
      " [-1.26088283  0.80051084  2.1221562   1.0324619  -1.65879717 -0.48423407\n",
      "   1.19367675 -0.70766947  0.44319828  0.78892956]\n",
      " [-0.85985379 -0.85758347 -3.25446759 -1.06161905 -1.200757   -1.24845053\n",
      "   1.13397404 -1.43014138 -0.42791702  0.33322971]\n",
      " [ 1.54435033 -1.43586215  1.14156554 -0.01790446 -0.98150865  0.46128733\n",
      "   0.19550779 -0.60021688  0.13382339 -0.30670536]\n",
      " [ 0.11506297  0.66213067  1.58555624 -1.23816517  2.13303337 -1.95212541\n",
      "  -0.16668123  0.58831721  0.40008586 -0.79500128]\n",
      " [-0.20812225 -0.54506836 -0.58936476  0.8496021   0.29515314 -0.6929096\n",
      "   0.86728437  0.30729952  0.81286212  0.63579589]\n",
      " [-0.82899501 -0.56018104  0.74729361  0.61037027 -0.02090159  0.11732738\n",
      "   1.2772036  -0.59157139  0.54924917 -0.21152983]]\n",
      "Bias b4: [[ 0.29095715 -0.06328923 -0.05240079 -0.08766036 -0.0751952  -0.00566116\n",
      "  -0.04702668  0.          0.07793577  0.35952627]]\n",
      "Weight W5: [[-0.66797112]\n",
      " [ 0.93643003]\n",
      " [ 0.82417092]\n",
      " [ 0.82815269]\n",
      " [ 1.07310837]\n",
      " [-0.02147045]\n",
      " [-0.06897526]\n",
      " [-0.31026676]\n",
      " [ 0.59389459]\n",
      " [-0.88130217]]\n",
      "Bias b5: [[-0.64808241]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]).reshape(-1, 1)\n",
    "Y = np.array([17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "Y = (Y - np.mean(Y)) / np.std(Y)\n",
    "\n",
    "# Neural Network parameters\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, hidden_size)\n",
    "b2 = np.zeros((1, hidden_size))\n",
    "W3 = np.random.randn(hidden_size, hidden_size)\n",
    "b3 = np.zeros((1, hidden_size))\n",
    "W4 = np.random.randn(hidden_size, hidden_size)\n",
    "b4 = np.zeros((1, hidden_size))\n",
    "W5 = np.random.randn(hidden_size, output_size)\n",
    "b5 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = relu(Z3)\n",
    "    Z4 = np.dot(A3, W4) + b4\n",
    "    A4 = relu(Z4)\n",
    "    Z5 = np.dot(A4, W5) + b5\n",
    "    return Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5\n",
    "\n",
    "# Mean squared error loss\n",
    "def compute_loss(Y, Y_pred):\n",
    "    return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "# Backward propagation\n",
    "def backward(X, Y, Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5):\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    dZ5 = Z5 - Y\n",
    "    dW5 = np.dot(A4.T, dZ5) / m\n",
    "    db5 = np.sum(dZ5, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA4 = np.dot(dZ5, W5.T)\n",
    "    dZ4 = dA4 * relu_derivative(Z4)\n",
    "    dW4 = np.dot(A3.T, dZ4) / m\n",
    "    db4 = np.sum(dZ4, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA3 = np.dot(dZ4, W4.T)\n",
    "    dZ3 = dA3 * relu_derivative(Z3)\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA2 = np.dot(dZ3, W3.T)\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5 = forward(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(Y, Z5)\n",
    "    \n",
    "    # Backward pass\n",
    "    dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5 = backward(X, Y, Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    W4 -= learning_rate * dW4\n",
    "    b4 -= learning_rate * db4\n",
    "    W5 -= learning_rate * dW5\n",
    "    b5 -= learning_rate * db5\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Results\n",
    "print(\"Weight W1:\", W1)\n",
    "print(\"Bias b1:\", b1)\n",
    "print(\"Weight W2:\", W2)\n",
    "print(\"Bias b2:\", b2)\n",
    "print(\"Weight W3:\", W3)\n",
    "print(\"Bias b3:\", b3)\n",
    "print(\"Weight W4:\", W4)\n",
    "print(\"Bias b4:\", b4)\n",
    "print(\"Weight W5:\", W5)\n",
    "print(\"Bias b5:\", b5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a24d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py): started\n",
      "  Building wheel for fpdf (setup.py): finished with status 'done'\n",
      "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40713 sha256=87fdac3482530be11233fb2f88bf90913036bc4d09424c33422269b4b92e82dc\n",
      "  Stored in directory: c:\\users\\ratho\\appdata\\local\\pip\\cache\\wheels\\65\\4f\\66\\bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    }
   ],
   "source": [
    "pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212b0dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points (N): 10\n",
      "Sum of X (sum_x): 73.65\n",
      "Sum of Y (sum_y): 173.5\n",
      "Sum of X*Y (sum_xy): 1286.2600000000002\n",
      "Sum of X^2 (sum_x2): 562.4355\n",
      "Slope (m): 0.42155649706923126\n",
      "Intercept (b): 14.245236399085083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "X = [5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]\n",
    "Y = [17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]\n",
    "\n",
    "# Number of data points\n",
    "N = len(X)\n",
    "\n",
    "# Calculate sums\n",
    "sum_x = sum(X)\n",
    "sum_y = sum(Y)\n",
    "sum_xy = sum(x * y for x, y in zip(X, Y))\n",
    "sum_x2 = sum(x ** 2 for x in X)\n",
    "\n",
    "# Calculate slope (m) and intercept (b)\n",
    "m = (N * sum_xy - sum_x * sum_y) / (N * sum_x2 - sum_x ** 2)\n",
    "b = (sum_y * sum_x2 - sum_x * sum_xy) / (N * sum_x2 - sum_x ** 2)\n",
    "\n",
    "# Print all necessary steps\n",
    "print(f\"Number of data points (N): {N}\")\n",
    "print(f\"Sum of X (sum_x): {sum_x}\")\n",
    "print(f\"Sum of Y (sum_y): {sum_y}\")\n",
    "print(f\"Sum of X*Y (sum_xy): {sum_xy}\")\n",
    "print(f\"Sum of X^2 (sum_x2): {sum_x2}\")\n",
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Intercept (b): {b}\")\n",
    "\n",
    "# Directly convert to PDF (This part of the code can be adapted to use a library like FPDF in Python)\n",
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Linear Regression Calculation', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(10)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.chapter_title('Calculation Steps and Results')\n",
    "\n",
    "body = (\n",
    "    f\"Number of data points (N): {N}\\n\"\n",
    "    f\"Sum of X (sum_x): {sum_x}\\n\"\n",
    "    f\"Sum of Y (sum_y): {sum_y}\\n\"\n",
    "    f\"Sum of X*Y (sum_xy): {sum_xy}\\n\"\n",
    "    f\"Sum of X^2 (sum_x2): {sum_x2}\\n\"\n",
    "    f\"Slope (m): {m}\\n\"\n",
    "    f\"Intercept (b): {b}\\n\"\n",
    ")\n",
    "\n",
    "pdf.chapter_body(body)\n",
    "\n",
    "pdf.output('linear_regression_calculation.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdda8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 49.155303949354895\n",
      "Epoch 2, Loss: 28.66082167141879\n",
      "Epoch 3, Loss: 16.717547084011684\n",
      "Epoch 4, Loss: 9.757493704912605\n",
      "Epoch 5, Loss: 5.701415646732819\n",
      "Epoch 6, Loss: 3.337631338152262\n",
      "Epoch 7, Loss: 1.9600327410426472\n",
      "Epoch 8, Loss: 1.1571350431752154\n",
      "Epoch 9, Loss: 0.6891452625838447\n",
      "Epoch 10, Loss: 0.41632391611520625\n",
      "Epoch 11, Loss: 0.2572377464368666\n",
      "Epoch 12, Loss: 0.16443147810705577\n",
      "Epoch 13, Loss: 0.11025052142296099\n",
      "Epoch 14, Loss: 0.07857914609442679\n",
      "Epoch 15, Loss: 0.06002589787653033\n",
      "Epoch 16, Loss: 0.049117897736966704\n",
      "Epoch 17, Loss: 0.042665818311583156\n",
      "Epoch 18, Loss: 0.03881107312708021\n",
      "Epoch 19, Loss: 0.036470565650317574\n",
      "Epoch 20, Loss: 0.03501311652268947\n",
      "Epoch 21, Loss: 0.034070901371142506\n",
      "Epoch 22, Loss: 0.03342956609932889\n",
      "Epoch 23, Loss: 0.03296419151912617\n",
      "Epoch 24, Loss: 0.032601976707871586\n",
      "Epoch 25, Loss: 0.03230049271618839\n",
      "Epoch 26, Loss: 0.03203500990530446\n",
      "Epoch 27, Loss: 0.031791112946431145\n",
      "Epoch 28, Loss: 0.03156039720782048\n",
      "Epoch 29, Loss: 0.03133796082201137\n",
      "Epoch 30, Loss: 0.03112094318659382\n",
      "Epoch 31, Loss: 0.030907673274623522\n",
      "Epoch 32, Loss: 0.030697173311911675\n",
      "Epoch 33, Loss: 0.030488869544537212\n",
      "Epoch 34, Loss: 0.030282423688292474\n",
      "Epoch 35, Loss: 0.03007763470558153\n",
      "Epoch 36, Loss: 0.029874381565682607\n",
      "Epoch 37, Loss: 0.029672589888083654\n",
      "Epoch 38, Loss: 0.029472212503693536\n",
      "Epoch 39, Loss: 0.029273218126702437\n",
      "Epoch 40, Loss: 0.02907558475292915\n",
      "Epoch 41, Loss: 0.02887929581253142\n",
      "Epoch 42, Loss: 0.02868433792782383\n",
      "Epoch 43, Loss: 0.028490699606474813\n",
      "Epoch 44, Loss: 0.028298370479796726\n",
      "Epoch 45, Loss: 0.028107340858690523\n",
      "Epoch 46, Loss: 0.027917601474705185\n",
      "Epoch 47, Loss: 0.027729143328973454\n",
      "Epoch 48, Loss: 0.027541957604014365\n",
      "Epoch 49, Loss: 0.027356035612171502\n",
      "Epoch 50, Loss: 0.027171368765404595\n",
      "Epoch 51, Loss: 0.026987948557519948\n",
      "Epoch 52, Loss: 0.026805766553659082\n",
      "Epoch 53, Loss: 0.026624814384010088\n",
      "Epoch 54, Loss: 0.02644508373998672\n",
      "Epoch 55, Loss: 0.02626656637184263\n",
      "Epoch 56, Loss: 0.02608925408712622\n",
      "Epoch 57, Loss: 0.025913138749623855\n",
      "Epoch 58, Loss: 0.025738212278590633\n",
      "Epoch 59, Loss: 0.02556446664814786\n",
      "Epoch 60, Loss: 0.025391893886780186\n",
      "Epoch 61, Loss: 0.025220486076892784\n",
      "Epoch 62, Loss: 0.02505023535440042\n",
      "Epoch 63, Loss: 0.024881133908341103\n",
      "Epoch 64, Loss: 0.024713173980502306\n",
      "Epoch 65, Loss: 0.024546347865055516\n",
      "Epoch 66, Loss: 0.024380647908197852\n",
      "Epoch 67, Loss: 0.02421606650779714\n",
      "Epoch 68, Loss: 0.02405259611304213\n",
      "Epoch 69, Loss: 0.02389022922409488\n",
      "Epoch 70, Loss: 0.02372895839174529\n",
      "Epoch 71, Loss: 0.02356877621707018\n",
      "Epoch 72, Loss: 0.02340967535109279\n",
      "Epoch 73, Loss: 0.02325164849444579\n",
      "Epoch 74, Loss: 0.0230946883970361\n",
      "Epoch 75, Loss: 0.022938787857712833\n",
      "Epoch 76, Loss: 0.022783939723935963\n",
      "Epoch 77, Loss: 0.02263013689144915\n",
      "Epoch 78, Loss: 0.022477372303952536\n",
      "Epoch 79, Loss: 0.022325638952780603\n",
      "Epoch 80, Loss: 0.0221749298765792\n",
      "Epoch 81, Loss: 0.022025238160987344\n",
      "Epoch 82, Loss: 0.021876556938318928\n",
      "Epoch 83, Loss: 0.021728879387248202\n",
      "Epoch 84, Loss: 0.021582198732496585\n",
      "Epoch 85, Loss: 0.021436508244522378\n",
      "Epoch 86, Loss: 0.02129180123921135\n",
      "Epoch 87, Loss: 0.021148071077570232\n",
      "Epoch 88, Loss: 0.02100531116542262\n",
      "Epoch 89, Loss: 0.020863514953105837\n",
      "Epoch 90, Loss: 0.02072267593517011\n",
      "Epoch 91, Loss: 0.020582787650081834\n",
      "Epoch 92, Loss: 0.02044384367992489\n",
      "Epoch 93, Loss: 0.02030583765010769\n",
      "Epoch 94, Loss: 0.020168763229070397\n",
      "Epoch 95, Loss: 0.020032614127994165\n",
      "Epoch 96, Loss: 0.019897384100512748\n",
      "Epoch 97, Loss: 0.01976306694242585\n",
      "Epoch 98, Loss: 0.019629656491415043\n",
      "Epoch 99, Loss: 0.019497146626760043\n",
      "Epoch 100, Loss: 0.01936553126905884\n",
      "Epoch 101, Loss: 0.01923480437994809\n",
      "Epoch 102, Loss: 0.019104959961826668\n",
      "Epoch 103, Loss: 0.018975992057579966\n",
      "Epoch 104, Loss: 0.018847894750306904\n",
      "Epoch 105, Loss: 0.018720662163048712\n",
      "Epoch 106, Loss: 0.018594288458518442\n",
      "Epoch 107, Loss: 0.018468767838834003\n",
      "Epoch 108, Loss: 0.01834409454525208\n",
      "Epoch 109, Loss: 0.018220262857903208\n",
      "Epoch 110, Loss: 0.01809726709553049\n",
      "Epoch 111, Loss: 0.017975101615227543\n",
      "Epoch 112, Loss: 0.017853760812181145\n",
      "Epoch 113, Loss: 0.01773323911941294\n",
      "Epoch 114, Loss: 0.017613531007524365\n",
      "Epoch 115, Loss: 0.017494630984443295\n",
      "Epoch 116, Loss: 0.01737653359517153\n",
      "Epoch 117, Loss: 0.017259233421534965\n",
      "Epoch 118, Loss: 0.017142725081934842\n",
      "Epoch 119, Loss: 0.017027003231100693\n",
      "Epoch 120, Loss: 0.01691206255984553\n",
      "Epoch 121, Loss: 0.016797897794821676\n",
      "Epoch 122, Loss: 0.016684503698279344\n",
      "Epoch 123, Loss: 0.016571875067826224\n",
      "Epoch 124, Loss: 0.016460006736188488\n",
      "Epoch 125, Loss: 0.016348893570974046\n",
      "Epoch 126, Loss: 0.016238530474437105\n",
      "Epoch 127, Loss: 0.016128912383244036\n",
      "Epoch 128, Loss: 0.0160200342682415\n",
      "Epoch 129, Loss: 0.015911891134224802\n",
      "Epoch 130, Loss: 0.015804478019709876\n",
      "Epoch 131, Loss: 0.015697789996704997\n",
      "Epoch 132, Loss: 0.01559182217048465\n",
      "Epoch 133, Loss: 0.015486569679365509\n",
      "Epoch 134, Loss: 0.015382027694482547\n",
      "Epoch 135, Loss: 0.015278191419568293\n",
      "Epoch 136, Loss: 0.015175056090732313\n",
      "Epoch 137, Loss: 0.015072616976242672\n",
      "Epoch 138, Loss: 0.014970869376309445\n",
      "Epoch 139, Loss: 0.014869808622867834\n",
      "Epoch 140, Loss: 0.014769430079365348\n",
      "Epoch 141, Loss: 0.014669729140548399\n",
      "Epoch 142, Loss: 0.014570701232251081\n",
      "Epoch 143, Loss: 0.01447234181118551\n",
      "Epoch 144, Loss: 0.01437464636473285\n",
      "Epoch 145, Loss: 0.014277610410736963\n",
      "Epoch 146, Loss: 0.01418122949729871\n",
      "Epoch 147, Loss: 0.014085499202570937\n",
      "Epoch 148, Loss: 0.013990415134556473\n",
      "Epoch 149, Loss: 0.01389597293090627\n",
      "Epoch 150, Loss: 0.013802168258719105\n",
      "Epoch 151, Loss: 0.013708996814343111\n",
      "Epoch 152, Loss: 0.013616454323178359\n",
      "Epoch 153, Loss: 0.01352453653948039\n",
      "Epoch 154, Loss: 0.013433239246165454\n",
      "Epoch 155, Loss: 0.013342558254617557\n",
      "Epoch 156, Loss: 0.013252489404495568\n",
      "Epoch 157, Loss: 0.013163028563543015\n",
      "Epoch 158, Loss: 0.013074171627398112\n",
      "Epoch 159, Loss: 0.012985914519405301\n",
      "Epoch 160, Loss: 0.012898253190428949\n",
      "Epoch 161, Loss: 0.012811183618666757\n",
      "Epoch 162, Loss: 0.012724701809465342\n",
      "Epoch 163, Loss: 0.012638803795137595\n",
      "Epoch 164, Loss: 0.012553485634780221\n",
      "Epoch 165, Loss: 0.012468743414092877\n",
      "Epoch 166, Loss: 0.012384573245198508\n",
      "Epoch 167, Loss: 0.012300971266465583\n",
      "Epoch 168, Loss: 0.012217933642330063\n",
      "Epoch 169, Loss: 0.012135456563120033\n",
      "Epoch 170, Loss: 0.012053536244880734\n",
      "Epoch 171, Loss: 0.011972168929200926\n",
      "Epoch 172, Loss: 0.01189135088304052\n",
      "Epoch 173, Loss: 0.011811078398559502\n",
      "Epoch 174, Loss: 0.011731347792947107\n",
      "Epoch 175, Loss: 0.011652155408253905\n",
      "Epoch 176, Loss: 0.011573497611223102\n",
      "Epoch 177, Loss: 0.011495370793124297\n",
      "Epoch 178, Loss: 0.011417771369587677\n",
      "Epoch 179, Loss: 0.011340695780439735\n",
      "Epoch 180, Loss: 0.011264140489540253\n",
      "Epoch 181, Loss: 0.011188101984619097\n",
      "Epoch 182, Loss: 0.01111257677711593\n",
      "Epoch 183, Loss: 0.011037561402020241\n",
      "Epoch 184, Loss: 0.01096305241771159\n",
      "Epoch 185, Loss: 0.010889046405802454\n",
      "Epoch 186, Loss: 0.010815539970980965\n",
      "Epoch 187, Loss: 0.010742529740855353\n",
      "Epoch 188, Loss: 0.010670012365799115\n",
      "Epoch 189, Loss: 0.010597984518797402\n",
      "Epoch 190, Loss: 0.01052644289529448\n",
      "Epoch 191, Loss: 0.010455384213041785\n",
      "Epoch 192, Loss: 0.01038480521194755\n",
      "Epoch 193, Loss: 0.010314702653927449\n",
      "Epoch 194, Loss: 0.010245073322755757\n",
      "Epoch 195, Loss: 0.010175914023917668\n",
      "Epoch 196, Loss: 0.010107221584463137\n",
      "Epoch 197, Loss: 0.010038992852861035\n",
      "Epoch 198, Loss: 0.009971224698854489\n",
      "Epoch 199, Loss: 0.00990391401331774\n",
      "Epoch 200, Loss: 0.009837057708112718\n",
      "Epoch 201, Loss: 0.009770652715948147\n",
      "Epoch 202, Loss: 0.009704695990238387\n",
      "Epoch 203, Loss: 0.009639184504963694\n",
      "Epoch 204, Loss: 0.00957411525453156\n",
      "Epoch 205, Loss: 0.009509485253638619\n",
      "Epoch 206, Loss: 0.009445291537133617\n",
      "Epoch 207, Loss: 0.009381531159882071\n",
      "Epoch 208, Loss: 0.00931820119662995\n",
      "Epoch 209, Loss: 0.009255298741870584\n",
      "Epoch 210, Loss: 0.009192820909710752\n",
      "Epoch 211, Loss: 0.009130764833738333\n",
      "Epoch 212, Loss: 0.009069127666891125\n",
      "Epoch 213, Loss: 0.009007906581325806\n",
      "Epoch 214, Loss: 0.00894709876828852\n",
      "Epoch 215, Loss: 0.008886701437985837\n",
      "Epoch 216, Loss: 0.008826711819456738\n",
      "Epoch 217, Loss: 0.00876712716044571\n",
      "Epoch 218, Loss: 0.008707944727276247\n",
      "Epoch 219, Loss: 0.008649161804725414\n",
      "Epoch 220, Loss: 0.008590775695899541\n",
      "Epoch 221, Loss: 0.008532783722110103\n",
      "Epoch 222, Loss: 0.008475183222751308\n",
      "Epoch 223, Loss: 0.00841797155517755\n",
      "Epoch 224, Loss: 0.00836114609458257\n",
      "Epoch 225, Loss: 0.008304704233878777\n",
      "Epoch 226, Loss: 0.008248643383577467\n",
      "Epoch 227, Loss: 0.008192960971670583\n",
      "Epoch 228, Loss: 0.008137654443512367\n",
      "Epoch 229, Loss: 0.0080827212617019\n",
      "Epoch 230, Loss: 0.00802815890596741\n",
      "Epoch 231, Loss: 0.007973964873049638\n",
      "Epoch 232, Loss: 0.007920136676588202\n",
      "Epoch 233, Loss: 0.007866671847006372\n",
      "Epoch 234, Loss: 0.007813567931398455\n",
      "Epoch 235, Loss: 0.0077608224934172295\n",
      "Epoch 236, Loss: 0.0077084331131616985\n",
      "Epoch 237, Loss: 0.007656397387066557\n",
      "Epoch 238, Loss: 0.00760471292779204\n",
      "Epoch 239, Loss: 0.00755337736411351\n",
      "Epoch 240, Loss: 0.007502388340813797\n",
      "Epoch 241, Loss: 0.007451743518574305\n",
      "Epoch 242, Loss: 0.007401440573868065\n",
      "Epoch 243, Loss: 0.007351477198853093\n",
      "Epoch 244, Loss: 0.007301851101266564\n",
      "Epoch 245, Loss: 0.0072525600043192836\n",
      "Epoch 246, Loss: 0.007203601646591834\n",
      "Epoch 247, Loss: 0.0071549737819301875\n",
      "Epoch 248, Loss: 0.007106674179343303\n",
      "Epoch 249, Loss: 0.007058700622900085\n",
      "Epoch 250, Loss: 0.00701105091162831\n",
      "Epoch 251, Loss: 0.006963722859413252\n",
      "Epoch 252, Loss: 0.0069167142948976575\n",
      "Epoch 253, Loss: 0.006870023061381955\n",
      "Epoch 254, Loss: 0.006823647016725212\n",
      "Epoch 255, Loss: 0.006777584033247433\n",
      "Epoch 256, Loss: 0.006731831997631055\n",
      "Epoch 257, Loss: 0.006686388810824578\n",
      "Epoch 258, Loss: 0.006641252387946275\n",
      "Epoch 259, Loss: 0.006596420658188185\n",
      "Epoch 260, Loss: 0.006551891564721596\n",
      "Epoch 261, Loss: 0.006507663064602103\n",
      "Epoch 262, Loss: 0.006463733128676515\n",
      "Epoch 263, Loss: 0.006420099741489027\n",
      "Epoch 264, Loss: 0.006376760901189488\n",
      "Epoch 265, Loss: 0.006333714619441026\n",
      "Epoch 266, Loss: 0.006290958921329308\n",
      "Epoch 267, Loss: 0.006248491845271093\n",
      "Epoch 268, Loss: 0.006206311442925551\n",
      "Epoch 269, Loss: 0.006164415779103324\n",
      "Epoch 270, Loss: 0.0061228029316792165\n",
      "Epoch 271, Loss: 0.006081470991502848\n",
      "Epoch 272, Loss: 0.006040418062311807\n",
      "Epoch 273, Loss: 0.005999642260644202\n",
      "Epoch 274, Loss: 0.005959141715752672\n",
      "Epoch 275, Loss: 0.005918914569518073\n",
      "Epoch 276, Loss: 0.005878958976364697\n",
      "Epoch 277, Loss: 0.005839273103175318\n",
      "Epoch 278, Loss: 0.005799855129207082\n",
      "Epoch 279, Loss: 0.00576070324600793\n",
      "Epoch 280, Loss: 0.00572181565733418\n",
      "Epoch 281, Loss: 0.005683190579067172\n",
      "Epoch 282, Loss: 0.005644826239132423\n",
      "Epoch 283, Loss: 0.005606720877417304\n",
      "Epoch 284, Loss: 0.0055688727456912005\n",
      "Epoch 285, Loss: 0.005531280107524858\n",
      "Epoch 286, Loss: 0.005493941238210568\n",
      "Epoch 287, Loss: 0.0054568544246835216\n",
      "Epoch 288, Loss: 0.005420017965442713\n",
      "Epoch 289, Loss: 0.005383430170473242\n",
      "Epoch 290, Loss: 0.005347089361168678\n",
      "Epoch 291, Loss: 0.005310993870253882\n",
      "Epoch 292, Loss: 0.005275142041708761\n",
      "Epoch 293, Loss: 0.005239532230691995\n",
      "Epoch 294, Loss: 0.005204162803466015\n",
      "Epoch 295, Loss: 0.00516903213732156\n",
      "Epoch 296, Loss: 0.00513413862050362\n",
      "Epoch 297, Loss: 0.00509948065213721\n",
      "Epoch 298, Loss: 0.005065056642154111\n",
      "Epoch 299, Loss: 0.005030865011219826\n",
      "Epoch 300, Loss: 0.004996904190661098\n",
      "Epoch 301, Loss: 0.00496317262239414\n",
      "Epoch 302, Loss: 0.004929668758852712\n",
      "Epoch 303, Loss: 0.004896391062917686\n",
      "Epoch 304, Loss: 0.004863338007846202\n",
      "Epoch 305, Loss: 0.004830508077201626\n",
      "Epoch 306, Loss: 0.0047978997647838886\n",
      "Epoch 307, Loss: 0.004765511574560675\n",
      "Epoch 308, Loss: 0.004733342020598662\n",
      "Epoch 309, Loss: 0.00470138962699517\n",
      "Epoch 310, Loss: 0.004669652927810658\n",
      "Epoch 311, Loss: 0.004638130467001412\n",
      "Epoch 312, Loss: 0.004606820798352701\n",
      "Epoch 313, Loss: 0.0045757224854124316\n",
      "Epoch 314, Loss: 0.004544834101425417\n",
      "Epoch 315, Loss: 0.004514154229267543\n",
      "Epoch 316, Loss: 0.004483681461381188\n",
      "Epoch 317, Loss: 0.004453414399710284\n",
      "Epoch 318, Loss: 0.0044233516556365085\n",
      "Epoch 319, Loss: 0.004393491849915197\n",
      "Epoch 320, Loss: 0.004363833612612373\n",
      "Epoch 321, Loss: 0.004334375583041826\n",
      "Epoch 322, Loss: 0.004305116409702539\n",
      "Epoch 323, Loss: 0.004276054750216874\n",
      "Epoch 324, Loss: 0.004247189271268954\n",
      "Epoch 325, Loss: 0.004218518648543251\n",
      "Epoch 326, Loss: 0.004190041566664256\n",
      "Epoch 327, Loss: 0.004161756719135714\n",
      "Epoch 328, Loss: 0.004133662808280948\n",
      "Epoch 329, Loss: 0.004105758545183215\n",
      "Epoch 330, Loss: 0.004078042649626551\n",
      "Epoch 331, Loss: 0.004050513850037279\n",
      "Epoch 332, Loss: 0.004023170883425183\n",
      "Epoch 333, Loss: 0.003996012495326072\n",
      "Epoch 334, Loss: 0.003969037439743887\n",
      "Epoch 335, Loss: 0.0039422444790936945\n",
      "Epoch 336, Loss: 0.003915632384144915\n",
      "Epoch 337, Loss: 0.003889199933964811\n",
      "Epoch 338, Loss: 0.003862945915862588\n",
      "Epoch 339, Loss: 0.003836869125333685\n",
      "Epoch 340, Loss: 0.0038109683660046537\n",
      "Epoch 341, Loss: 0.0037852424495778947\n",
      "Epoch 342, Loss: 0.0037596901957777083\n",
      "Epoch 343, Loss: 0.0037343104322956336\n",
      "Epoch 344, Loss: 0.003709101994736818\n",
      "Epoch 345, Loss: 0.0036840637265670093\n",
      "Epoch 346, Loss: 0.003659194479058718\n",
      "Epoch 347, Loss: 0.0036344931112391584\n",
      "Epoch 348, Loss: 0.0036099584898375745\n",
      "Epoch 349, Loss: 0.0035855894892334464\n",
      "Epoch 350, Loss: 0.003561384991404663\n",
      "Epoch 351, Loss: 0.0035373438858763056\n",
      "Epoch 352, Loss: 0.0035134650696697447\n",
      "Epoch 353, Loss: 0.0034897474472520883\n",
      "Epoch 354, Loss: 0.0034661899304857095\n",
      "Epoch 355, Loss: 0.0034427914385784285\n",
      "Epoch 356, Loss: 0.0034195508980339865\n",
      "Epoch 357, Loss: 0.0033964672426028872\n",
      "Epoch 358, Loss: 0.003373539413233083\n",
      "Epoch 359, Loss: 0.0033507663580219205\n",
      "Epoch 360, Loss: 0.0033281470321673704\n",
      "Epoch 361, Loss: 0.003305680397920547\n",
      "Epoch 362, Loss: 0.00328336542453759\n",
      "Epoch 363, Loss: 0.003261201088233071\n",
      "Epoch 364, Loss: 0.0032391863721322814\n",
      "Epoch 365, Loss: 0.0032173202662250794\n",
      "Epoch 366, Loss: 0.003195601767319324\n",
      "Epoch 367, Loss: 0.0031740298789949576\n",
      "Epoch 368, Loss: 0.003152603611558214\n",
      "Epoch 369, Loss: 0.003131321981996295\n",
      "Epoch 370, Loss: 0.0031101840139321642\n",
      "Epoch 371, Loss: 0.003089188737579826\n",
      "Epoch 372, Loss: 0.0030683351896999096\n",
      "Epoch 373, Loss: 0.0030476224135552415\n",
      "Epoch 374, Loss: 0.0030270494588671803\n",
      "Epoch 375, Loss: 0.003006615381772008\n",
      "Epoch 376, Loss: 0.0029863192447772955\n",
      "Epoch 377, Loss: 0.0029661601167194395\n",
      "Epoch 378, Loss: 0.0029461370727204804\n",
      "Epoch 379, Loss: 0.0029262491941460045\n",
      "Epoch 380, Loss: 0.00290649556856258\n",
      "Epoch 381, Loss: 0.0028868752896962857\n",
      "Epoch 382, Loss: 0.002867387457391097\n",
      "Epoch 383, Loss: 0.00284803117756731\n",
      "Epoch 384, Loss: 0.0028288055621808863\n",
      "Epoch 385, Loss: 0.002809709729182282\n",
      "Epoch 386, Loss: 0.002790742802476403\n",
      "Epoch 387, Loss: 0.0027719039118822572\n",
      "Epoch 388, Loss: 0.002753192193092802\n",
      "Epoch 389, Loss: 0.0027346067876357436\n",
      "Epoch 390, Loss: 0.0027161468428337126\n",
      "Epoch 391, Loss: 0.002697811511765412\n",
      "Epoch 392, Loss: 0.002679599953226655\n",
      "Epoch 393, Loss: 0.0026615113316919028\n",
      "Epoch 394, Loss: 0.0026435448172755483\n",
      "Epoch 395, Loss: 0.002625699585694477\n",
      "Epoch 396, Loss: 0.002607974818229647\n",
      "Epoch 397, Loss: 0.002590369701688856\n",
      "Epoch 398, Loss: 0.0025728834283693285\n",
      "Epoch 399, Loss: 0.0025555151960207126\n",
      "Epoch 400, Loss: 0.00253826420780826\n",
      "Epoch 401, Loss: 0.002521129672276137\n",
      "Epoch 402, Loss: 0.002504110803311324\n",
      "Epoch 403, Loss: 0.0024872068201074937\n",
      "Epoch 404, Loss: 0.002470416947129068\n",
      "Epoch 405, Loss: 0.0024537404140757254\n",
      "Epoch 406, Loss: 0.002437176455847033\n",
      "Epoch 407, Loss: 0.0024207243125073776\n",
      "Epoch 408, Loss: 0.0024043832292511827\n",
      "Epoch 409, Loss: 0.002388152456367953\n",
      "Epoch 410, Loss: 0.0023720312492084487\n",
      "Epoch 411, Loss: 0.002356018868149867\n",
      "Epoch 412, Loss: 0.0023401145785623073\n",
      "Epoch 413, Loss: 0.0023243176507751055\n",
      "Epoch 414, Loss: 0.002308627360043116\n",
      "Epoch 415, Loss: 0.0022930429865136385\n",
      "Epoch 416, Loss: 0.002277563815193258\n",
      "Epoch 417, Loss: 0.002262189135915122\n",
      "Epoch 418, Loss: 0.002246918243306547\n",
      "Epoch 419, Loss: 0.002231750436756217\n",
      "Epoch 420, Loss: 0.0022166850203824127\n",
      "Epoch 421, Loss: 0.0022017213030008686\n",
      "Epoch 422, Loss: 0.002186858598093334\n",
      "Epoch 423, Loss: 0.0021720962237756743\n",
      "Epoch 424, Loss: 0.002157433502767027\n",
      "Epoch 425, Loss: 0.0021428697623582663\n",
      "Epoch 426, Loss: 0.0021284043343815988\n",
      "Epoch 427, Loss: 0.0021140365551795643\n",
      "Epoch 428, Loss: 0.0020997657655748214\n",
      "Epoch 429, Loss: 0.0020855913108397154\n",
      "Epoch 430, Loss: 0.0020715125406663678\n",
      "Epoch 431, Loss: 0.0020575288091367167\n",
      "Epoch 432, Loss: 0.0020436394746931015\n",
      "Epoch 433, Loss: 0.002029843900108614\n",
      "Epoch 434, Loss: 0.0020161414524579634\n",
      "Epoch 435, Loss: 0.0020025315030884052\n",
      "Epoch 436, Loss: 0.00198901342759087\n",
      "Epoch 437, Loss: 0.001975586605771416\n",
      "Epoch 438, Loss: 0.0019622504216227398\n",
      "Epoch 439, Loss: 0.0019490042632957904\n",
      "Epoch 440, Loss: 0.0019358475230719639\n",
      "Epoch 441, Loss: 0.00192277959733478\n",
      "Epoch 442, Loss: 0.001909799886542863\n",
      "Epoch 443, Loss: 0.001896907795201634\n",
      "Epoch 444, Loss: 0.0018841027318366593\n",
      "Epoch 445, Loss: 0.001871384108966153\n",
      "Epoch 446, Loss: 0.001858751343074175\n",
      "Epoch 447, Loss: 0.001846203854583704\n",
      "Epoch 448, Loss: 0.0018337410678302226\n",
      "Epoch 449, Loss: 0.001821362411035233\n",
      "Epoch 450, Loss: 0.001809067316280049\n",
      "Epoch 451, Loss: 0.0017968552194796011\n",
      "Epoch 452, Loss: 0.0017847255603568829\n",
      "Epoch 453, Loss: 0.0017726777824167636\n",
      "Epoch 454, Loss: 0.00176071133292094\n",
      "Epoch 455, Loss: 0.0017488256628622914\n",
      "Epoch 456, Loss: 0.0017370202269397044\n",
      "Epoch 457, Loss: 0.0017252944835332718\n",
      "Epoch 458, Loss: 0.0017136478946791884\n",
      "Epoch 459, Loss: 0.0017020799260451052\n",
      "Epoch 460, Loss: 0.001690590046905842\n",
      "Epoch 461, Loss: 0.0016791777301187068\n",
      "Epoch 462, Loss: 0.001667842452099599\n",
      "Epoch 463, Loss: 0.001656583692798834\n",
      "Epoch 464, Loss: 0.0016454009356773325\n",
      "Epoch 465, Loss: 0.0016342936676828179\n",
      "Epoch 466, Loss: 0.0016232613792265128\n",
      "Epoch 467, Loss: 0.0016123035641595917\n",
      "Epoch 468, Loss: 0.001601419719749845\n",
      "Epoch 469, Loss: 0.0015906093466588448\n",
      "Epoch 470, Loss: 0.0015798719489189855\n",
      "Epoch 471, Loss: 0.0015692070339106575\n",
      "Epoch 472, Loss: 0.00155861411233971\n",
      "Epoch 473, Loss: 0.001548092698214793\n",
      "Epoch 474, Loss: 0.0015376423088255002\n",
      "Epoch 475, Loss: 0.0015272624647197429\n",
      "Epoch 476, Loss: 0.0015169526896821183\n",
      "Epoch 477, Loss: 0.0015067125107117948\n",
      "Epoch 478, Loss: 0.001496541458001035\n",
      "Epoch 479, Loss: 0.0014864390649134644\n",
      "Epoch 480, Loss: 0.0014764048679627624\n",
      "Epoch 481, Loss: 0.001466438406791389\n",
      "Epoch 482, Loss: 0.0014565392241493544\n",
      "Epoch 483, Loss: 0.0014467068658734349\n",
      "Epoch 484, Loss: 0.001436940880866177\n",
      "Epoch 485, Loss: 0.001427240821075297\n",
      "Epoch 486, Loss: 0.0014176062414731128\n",
      "Epoch 487, Loss: 0.001408036700035959\n",
      "Epoch 488, Loss: 0.0013985317577241674\n",
      "Epoch 489, Loss: 0.001389090978461805\n",
      "Epoch 490, Loss: 0.001379713929116593\n",
      "Epoch 491, Loss: 0.0013704001794801897\n",
      "Epoch 492, Loss: 0.0013611493022482935\n",
      "Epoch 493, Loss: 0.0013519608730012014\n",
      "Epoch 494, Loss: 0.0013428344701842006\n",
      "Epoch 495, Loss: 0.0013337696750882542\n",
      "Epoch 496, Loss: 0.0013247660718309169\n",
      "Epoch 497, Loss: 0.0013158232473370278\n",
      "Epoch 498, Loss: 0.0013069407913199813\n",
      "Epoch 499, Loss: 0.001298118296262826\n",
      "Epoch 500, Loss: 0.0012893553573994244\n",
      "Epoch 501, Loss: 0.0012806515726960861\n",
      "Epoch 502, Loss: 0.0012720065428331062\n",
      "Epoch 503, Loss: 0.0012634198711863355\n",
      "Epoch 504, Loss: 0.001254891163808954\n",
      "Epoch 505, Loss: 0.0012464200294136421\n",
      "Epoch 506, Loss: 0.001238006079354306\n",
      "Epoch 507, Loss: 0.0012296489276084571\n",
      "Epoch 508, Loss: 0.0012213481907595188\n",
      "Epoch 509, Loss: 0.001213103487979055\n",
      "Epoch 510, Loss: 0.0012049144410094745\n",
      "Epoch 511, Loss: 0.0011967806741466505\n",
      "Epoch 512, Loss: 0.0011887018142225409\n",
      "Epoch 513, Loss: 0.0011806774905881979\n",
      "Epoch 514, Loss: 0.0011727073350968325\n",
      "Epoch 515, Loss: 0.0011647909820867012\n",
      "Epoch 516, Loss: 0.0011569280683645057\n",
      "Epoch 517, Loss: 0.0011491182331887736\n",
      "Epoch 518, Loss: 0.0011413611182530836\n",
      "Epoch 519, Loss: 0.001133656367669779\n",
      "Epoch 520, Loss: 0.0011260036279536743\n",
      "Epoch 521, Loss: 0.0011184025480057706\n",
      "Epoch 522, Loss: 0.0011108527790971501\n",
      "Epoch 523, Loss: 0.0011033539748529378\n",
      "Epoch 524, Loss: 0.0010959057912365746\n",
      "Epoch 525, Loss: 0.0010885078865338123\n",
      "Epoch 526, Loss: 0.001081159921337197\n",
      "Epoch 527, Loss: 0.0010738615585304224\n",
      "Epoch 528, Loss: 0.0010666124632728795\n",
      "Epoch 529, Loss: 0.0010594123029843258\n",
      "Epoch 530, Loss: 0.0010522607473295554\n",
      "Epoch 531, Loss: 0.0010451574682033287\n",
      "Epoch 532, Loss: 0.0010381021397152983\n",
      "Epoch 533, Loss: 0.001031094438174928\n",
      "Epoch 534, Loss: 0.0010241340420768575\n",
      "Epoch 535, Loss: 0.0010172206320860446\n",
      "Epoch 536, Loss: 0.0010103538910230547\n",
      "Epoch 537, Loss: 0.001003533503849613\n",
      "Epoch 538, Loss: 0.0009967591576540995\n",
      "Epoch 539, Loss: 0.0009900305416371826\n",
      "Epoch 540, Loss: 0.0009833473470976517\n",
      "Epoch 541, Loss: 0.0009767092674180795\n",
      "Epoch 542, Loss: 0.0009701159980509128\n",
      "Epoch 543, Loss: 0.0009635672365044442\n",
      "Epoch 544, Loss: 0.0009570626823289071\n",
      "Epoch 545, Loss: 0.000950602037102792\n",
      "Epoch 546, Loss: 0.0009441850044189919\n",
      "Epoch 547, Loss: 0.000937811289871343\n",
      "Epoch 548, Loss: 0.0009314806010410412\n",
      "Epoch 549, Loss: 0.0009251926474832895\n",
      "Epoch 550, Loss: 0.0009189471407138952\n",
      "Epoch 551, Loss: 0.0009127437941961264\n",
      "Epoch 552, Loss: 0.000906582323327477\n",
      "Epoch 553, Loss: 0.0009004624454266306\n",
      "Epoch 554, Loss: 0.0008943838797205838\n",
      "Epoch 555, Loss: 0.0008883463473315977\n",
      "Epoch 556, Loss: 0.0008823495712645946\n",
      "Epoch 557, Loss: 0.0008763932763942285\n",
      "Epoch 558, Loss: 0.0008704771894524977\n",
      "Epoch 559, Loss: 0.0008646010390160518\n",
      "Epoch 560, Loss: 0.0008587645554937373\n",
      "Epoch 561, Loss: 0.000852967471114352\n",
      "Epoch 562, Loss: 0.000847209519914253\n",
      "Epoch 563, Loss: 0.0008414904377251562\n",
      "Epoch 564, Loss: 0.0008358099621620652\n",
      "Epoch 565, Loss: 0.00083016783261118\n",
      "Epoch 566, Loss: 0.0008245637902180791\n",
      "Epoch 567, Loss: 0.0008189975778755909\n",
      "Epoch 568, Loss: 0.0008134689402122433\n",
      "Epoch 569, Loss: 0.0008079776235804118\n",
      "Epoch 570, Loss: 0.000802523376044718\n",
      "Epoch 571, Loss: 0.0007971059473704951\n",
      "Epoch 572, Loss: 0.0007917250890121786\n",
      "Epoch 573, Loss: 0.0007863805541022449\n",
      "Epoch 574, Loss: 0.0007810720974393718\n",
      "Epoch 575, Loss: 0.0007757994754776718\n",
      "Epoch 576, Loss: 0.0007705624463152099\n",
      "Epoch 577, Loss: 0.0007653607696830018\n",
      "Epoch 578, Loss: 0.0007601942069340496\n",
      "Epoch 579, Loss: 0.0007550625210323741\n",
      "Epoch 580, Loss: 0.0007499654765419525\n",
      "Epoch 581, Loss: 0.000744902839616253\n",
      "Epoch 582, Loss: 0.0007398743779872092\n",
      "Epoch 583, Loss: 0.0007348798609546949\n",
      "Epoch 584, Loss: 0.0007299190593759371\n",
      "Epoch 585, Loss: 0.0007249917456550248\n",
      "Epoch 586, Loss: 0.0007200976937323659\n",
      "Epoch 587, Loss: 0.0007152366790744607\n",
      "Epoch 588, Loss: 0.0007104084786633958\n",
      "Epoch 589, Loss: 0.0007056128709869117\n",
      "Epoch 590, Loss: 0.0007008496360279009\n",
      "Epoch 591, Loss: 0.0006961185552545807\n",
      "Epoch 592, Loss: 0.0006914194116103137\n",
      "Epoch 593, Loss: 0.0006867519895037459\n",
      "Epoch 594, Loss: 0.0006821160747988537\n",
      "Epoch 595, Loss: 0.0006775114548051157\n",
      "Epoch 596, Loss: 0.0006729379182677777\n",
      "Epoch 597, Loss: 0.0006683952553582351\n",
      "Epoch 598, Loss: 0.0006638832576642337\n",
      "Epoch 599, Loss: 0.0006594017181804289\n",
      "Epoch 600, Loss: 0.0006549504312988968\n",
      "Epoch 601, Loss: 0.0006505291927996241\n",
      "Epoch 602, Loss: 0.0006461377998412226\n",
      "Epoch 603, Loss: 0.0006417760509515877\n",
      "Epoch 604, Loss: 0.0006374437460186257\n",
      "Epoch 605, Loss: 0.0006331406862810806\n",
      "Epoch 606, Loss: 0.0006288666743194636\n",
      "Epoch 607, Loss: 0.0006246215140469666\n",
      "Epoch 608, Loss: 0.0006204050107004395\n",
      "Epoch 609, Loss: 0.0006162169708315386\n",
      "Epoch 610, Loss: 0.0006120572022977045\n",
      "Epoch 611, Loss: 0.0006079255142534781\n",
      "Epoch 612, Loss: 0.0006038217171417336\n",
      "Epoch 613, Loss: 0.0005997456226849104\n",
      "Epoch 614, Loss: 0.0005956970438763903\n",
      "Epoch 615, Loss: 0.0005916757949720343\n",
      "Epoch 616, Loss: 0.0005876816914814247\n",
      "Epoch 617, Loss: 0.0005837145501597133\n",
      "Epoch 618, Loss: 0.0005797741889989157\n",
      "Epoch 619, Loss: 0.0005758604272197009\n",
      "Epoch 620, Loss: 0.0005719730852631938\n",
      "Epoch 621, Loss: 0.0005681119847825159\n",
      "Epoch 622, Loss: 0.0005642769486347672\n",
      "Epoch 623, Loss: 0.0005604678008728624\n",
      "Epoch 624, Loss: 0.0005566843667373924\n",
      "Epoch 625, Loss: 0.0005529264726487069\n",
      "Epoch 626, Loss: 0.0005491939461988872\n",
      "Epoch 627, Loss: 0.0005454866161438767\n",
      "Epoch 628, Loss: 0.0005418043123955971\n",
      "Epoch 629, Loss: 0.000538146866014063\n",
      "Epoch 630, Loss: 0.0005345141091998332\n",
      "Epoch 631, Loss: 0.0005309058752861552\n",
      "Epoch 632, Loss: 0.000527321998731336\n",
      "Epoch 633, Loss: 0.0005237623151111963\n",
      "Epoch 634, Loss: 0.0005202266611114925\n",
      "Epoch 635, Loss: 0.0005167148745204297\n",
      "Epoch 636, Loss: 0.0005132267942212258\n",
      "Epoch 637, Loss: 0.0005097622601847174\n",
      "Epoch 638, Loss: 0.0005063211134620131\n",
      "Epoch 639, Loss: 0.0005029031961771977\n",
      "Epoch 640, Loss: 0.0004995083515201327\n",
      "Epoch 641, Loss: 0.0004961364237391788\n",
      "Epoch 642, Loss: 0.0004927872581341229\n",
      "Epoch 643, Loss: 0.0004894607010490505\n",
      "Epoch 644, Loss: 0.0004861565998653003\n",
      "Epoch 645, Loss: 0.0004828748029944473\n",
      "Epoch 646, Loss: 0.00047961515987139474\n",
      "Epoch 647, Loss: 0.00047637752094741\n",
      "Epoch 648, Loss: 0.00047316173768329844\n",
      "Epoch 649, Loss: 0.00046996766254256666\n",
      "Epoch 650, Loss: 0.00046679514898468905\n",
      "Epoch 651, Loss: 0.00046364405145830754\n",
      "Epoch 652, Loss: 0.0004605142253946728\n",
      "Epoch 653, Loss: 0.00045740552720094\n",
      "Epoch 654, Loss: 0.00045431781425351453\n",
      "Epoch 655, Loss: 0.00045125094489167515\n",
      "Epoch 656, Loss: 0.0004482047784109346\n",
      "Epoch 657, Loss: 0.00044517917505657555\n",
      "Epoch 658, Loss: 0.00044217399601741337\n",
      "Epoch 659, Loss: 0.0004391891034192212\n",
      "Epoch 660, Loss: 0.00043622436031854814\n",
      "Epoch 661, Loss: 0.000433279630696283\n",
      "Epoch 662, Loss: 0.0004303547794516123\n",
      "Epoch 663, Loss: 0.0004274496723956681\n",
      "Epoch 664, Loss: 0.0004245641762454423\n",
      "Epoch 665, Loss: 0.0004216981586176838\n",
      "Epoch 666, Loss: 0.0004188514880227098\n",
      "Epoch 667, Loss: 0.00041602403385851097\n",
      "Epoch 668, Loss: 0.00041321566640469004\n",
      "Epoch 669, Loss: 0.0004104262568165818\n",
      "Epoch 670, Loss: 0.00040765567711919224\n",
      "Epoch 671, Loss: 0.00040490380020152285\n",
      "Epoch 672, Loss: 0.00040217049981050926\n",
      "Epoch 673, Loss: 0.0003994556505455173\n",
      "Epoch 674, Loss: 0.00039675912785227407\n",
      "Epoch 675, Loss: 0.00039408080801742195\n",
      "Epoch 676, Loss: 0.0003914205681626854\n",
      "Epoch 677, Loss: 0.0003887782862392516\n",
      "Epoch 678, Loss: 0.00038615384102223544\n",
      "Epoch 679, Loss: 0.0003835471121050851\n",
      "Epoch 680, Loss: 0.0003809579798940434\n",
      "Epoch 681, Loss: 0.0003783863256026262\n",
      "Epoch 682, Loss: 0.00037583203124628837\n",
      "Epoch 683, Loss: 0.00037329497963689676\n",
      "Epoch 684, Loss: 0.00037077505437737733\n",
      "Epoch 685, Loss: 0.000368272139856461\n",
      "Epoch 686, Loss: 0.00036578612124326857\n",
      "Epoch 687, Loss: 0.00036331688448205165\n",
      "Epoch 688, Loss: 0.00036086431628705223\n",
      "Epoch 689, Loss: 0.00035842830413722565\n",
      "Epoch 690, Loss: 0.00035600873627109953\n",
      "Epoch 691, Loss: 0.00035360550168164187\n",
      "Epoch 692, Loss: 0.0003512184901111874\n",
      "Epoch 693, Loss: 0.0003488475920463492\n",
      "Epoch 694, Loss: 0.00034649269871302277\n",
      "Epoch 695, Loss: 0.00034415370207134484\n",
      "Epoch 696, Loss: 0.0003418304948108545\n",
      "Epoch 697, Loss: 0.0003395229703454002\n",
      "Epoch 698, Loss: 0.00033723102280839727\n",
      "Epoch 699, Loss: 0.00033495454704789336\n",
      "Epoch 700, Loss: 0.00033269343862174635\n",
      "Epoch 701, Loss: 0.00033044759379290004\n",
      "Epoch 702, Loss: 0.0003282169095245231\n",
      "Epoch 703, Loss: 0.00032600128347534544\n",
      "Epoch 704, Loss: 0.00032380061399500925\n",
      "Epoch 705, Loss: 0.0003216148001192247\n",
      "Epoch 706, Loss: 0.00031944374156537864\n",
      "Epoch 707, Loss: 0.00031728733872774233\n",
      "Epoch 708, Loss: 0.0003151454926730175\n",
      "Epoch 709, Loss: 0.0003130181051357694\n",
      "Epoch 710, Loss: 0.0003109050785137997\n",
      "Epoch 711, Loss: 0.0003088063158639064\n",
      "Epoch 712, Loss: 0.0003067217208972452\n",
      "Epoch 713, Loss: 0.0003046511979749222\n",
      "Epoch 714, Loss: 0.00030259465210371385\n",
      "Epoch 715, Loss: 0.0003005519889316198\n",
      "Epoch 716, Loss: 0.0002985231147436002\n",
      "Epoch 717, Loss: 0.00029650793645721624\n",
      "Epoch 718, Loss: 0.0002945063616183259\n",
      "Epoch 719, Loss: 0.0002925182983969853\n",
      "Epoch 720, Loss: 0.00029054365558311095\n",
      "Epoch 721, Loss: 0.00028858234258231886\n",
      "Epoch 722, Loss: 0.00028663426941179834\n",
      "Epoch 723, Loss: 0.0002846993466961662\n",
      "Epoch 724, Loss: 0.00028277748566336955\n",
      "Epoch 725, Loss: 0.0002808685981405985\n",
      "Epoch 726, Loss: 0.0002789725965502755\n",
      "Epoch 727, Loss: 0.00027708939390596406\n",
      "Epoch 728, Loss: 0.00027521890380849894\n",
      "Epoch 729, Loss: 0.00027336104044191285\n",
      "Epoch 730, Loss: 0.0002715157185695328\n",
      "Epoch 731, Loss: 0.00026968285353008833\n",
      "Epoch 732, Loss: 0.0002678623612338295\n",
      "Epoch 733, Loss: 0.00026605415815859917\n",
      "Epoch 734, Loss: 0.00026425816134613655\n",
      "Epoch 735, Loss: 0.00026247428839810955\n",
      "Epoch 736, Loss: 0.00026070245747247844\n",
      "Epoch 737, Loss: 0.00025894258727964415\n",
      "Epoch 738, Loss: 0.0002571945970787495\n",
      "Epoch 739, Loss: 0.00025545840667399333\n",
      "Epoch 740, Loss: 0.00025373393641095737\n",
      "Epoch 741, Loss: 0.00025202110717285686\n",
      "Epoch 742, Loss: 0.00025031984037707597\n",
      "Epoch 743, Loss: 0.0002486300579713809\n",
      "Epoch 744, Loss: 0.00024695168243051203\n",
      "Epoch 745, Loss: 0.00024528463675247976\n",
      "Epoch 746, Loss: 0.00024362884445513835\n",
      "Epoch 747, Loss: 0.0002419842295726032\n",
      "Epoch 748, Loss: 0.00024035071665182557\n",
      "Epoch 749, Loss: 0.000238728230749063\n",
      "Epoch 750, Loss: 0.00023711669742653728\n",
      "Epoch 751, Loss: 0.000235516042748897\n",
      "Epoch 752, Loss: 0.00023392619327993386\n",
      "Epoch 753, Loss: 0.00023234707607915398\n",
      "Epoch 754, Loss: 0.00023077861869847267\n",
      "Epoch 755, Loss: 0.00022922074917883662\n",
      "Epoch 756, Loss: 0.00022767339604695338\n",
      "Epoch 757, Loss: 0.0002261364883120301\n",
      "Epoch 758, Loss: 0.00022460995546246757\n",
      "Epoch 759, Loss: 0.00022309372746267723\n",
      "Epoch 760, Loss: 0.0002215877347498332\n",
      "Epoch 761, Loss: 0.00022009190823072674\n",
      "Epoch 762, Loss: 0.00021860617927852094\n",
      "Epoch 763, Loss: 0.0002171304797296487\n",
      "Epoch 764, Loss: 0.00021566474188067962\n",
      "Epoch 765, Loss: 0.00021420889848525914\n",
      "Epoch 766, Loss: 0.0002127628827509269\n",
      "Epoch 767, Loss: 0.00021132662833612251\n",
      "Epoch 768, Loss: 0.0002099000693471231\n",
      "Epoch 769, Loss: 0.00020848314033503033\n",
      "Epoch 770, Loss: 0.00020707577629272734\n",
      "Epoch 771, Loss: 0.0002056779126519669\n",
      "Epoch 772, Loss: 0.00020428948528036415\n",
      "Epoch 773, Loss: 0.00020291043047842048\n",
      "Epoch 774, Loss: 0.0002015406849766828\n",
      "Epoch 775, Loss: 0.0002001801859327703\n",
      "Epoch 776, Loss: 0.0001988288709285621\n",
      "Epoch 777, Loss: 0.00019748667796722105\n",
      "Epoch 778, Loss: 0.00019615354547047496\n",
      "Epoch 779, Loss: 0.00019482941227572228\n",
      "Epoch 780, Loss: 0.00019351421763323236\n",
      "Epoch 781, Loss: 0.00019220790120336845\n",
      "Epoch 782, Loss: 0.00019091040305380276\n",
      "Epoch 783, Loss: 0.0001896216636568201\n",
      "Epoch 784, Loss: 0.00018834162388650059\n",
      "Epoch 785, Loss: 0.00018707022501606729\n",
      "Epoch 786, Loss: 0.00018580740871519666\n",
      "Epoch 787, Loss: 0.0001845531170473158\n",
      "Epoch 788, Loss: 0.00018330729246691908\n",
      "Epoch 789, Loss: 0.00018206987781702201\n",
      "Epoch 790, Loss: 0.00018084081632640115\n",
      "Epoch 791, Loss: 0.000179620051607163\n",
      "Epoch 792, Loss: 0.00017840752765198615\n",
      "Epoch 793, Loss: 0.00017720318883165267\n",
      "Epoch 794, Loss: 0.0001760069798924373\n",
      "Epoch 795, Loss: 0.0001748188459536739\n",
      "Epoch 796, Loss: 0.0001736387325051019\n",
      "Epoch 797, Loss: 0.00017246658540444603\n",
      "Epoch 798, Loss: 0.00017130235087496514\n",
      "Epoch 799, Loss: 0.00017014597550285053\n",
      "Epoch 800, Loss: 0.00016899740623494927\n",
      "Epoch 801, Loss: 0.0001678565903761982\n",
      "Epoch 802, Loss: 0.00016672347558726884\n",
      "Epoch 803, Loss: 0.00016559800988210765\n",
      "Epoch 804, Loss: 0.00016448014162566876\n",
      "Epoch 805, Loss: 0.0001633698195314049\n",
      "Epoch 806, Loss: 0.00016226699265900357\n",
      "Epoch 807, Loss: 0.0001611716104120069\n",
      "Epoch 808, Loss: 0.00016008362253554692\n",
      "Epoch 809, Loss: 0.00015900297911394974\n",
      "Epoch 810, Loss: 0.00015792963056852617\n",
      "Epoch 811, Loss: 0.00015686352765527663\n",
      "Epoch 812, Loss: 0.00015580462146259645\n",
      "Epoch 813, Loss: 0.00015475286340908525\n",
      "Epoch 814, Loss: 0.0001537082052412618\n",
      "Epoch 815, Loss: 0.0001526705990314035\n",
      "Epoch 816, Loss: 0.00015163999717531518\n",
      "Epoch 817, Loss: 0.00015061635239014566\n",
      "Epoch 818, Loss: 0.00014959961771225408\n",
      "Epoch 819, Loss: 0.00014858974649498705\n",
      "Epoch 820, Loss: 0.0001475866924066129\n",
      "Epoch 821, Loss: 0.00014659040942813478\n",
      "Epoch 822, Loss: 0.00014560085185122824\n",
      "Epoch 823, Loss: 0.00014461797427612518\n",
      "Epoch 824, Loss: 0.00014364173160950748\n",
      "Epoch 825, Loss: 0.00014267207906247333\n",
      "Epoch 826, Loss: 0.0001417089721484629\n",
      "Epoch 827, Loss: 0.00014075236668121107\n",
      "Epoch 828, Loss: 0.00013980221877276568\n",
      "Epoch 829, Loss: 0.00013885848483141374\n",
      "Epoch 830, Loss: 0.00013792112155971127\n",
      "Epoch 831, Loss: 0.00013699008595249819\n",
      "Epoch 832, Loss: 0.00013606533529491314\n",
      "Epoch 833, Loss: 0.00013514682716044116\n",
      "Epoch 834, Loss: 0.0001342345194089844\n",
      "Epoch 835, Loss: 0.00013332837018489502\n",
      "Epoch 836, Loss: 0.00013242833791506526\n",
      "Epoch 837, Loss: 0.0001315343813070495\n",
      "Epoch 838, Loss: 0.00013064645934713116\n",
      "Epoch 839, Loss: 0.00012976453129844752\n",
      "Epoch 840, Loss: 0.00012888855669915172\n",
      "Epoch 841, Loss: 0.0001280184953605147\n",
      "Epoch 842, Loss: 0.00012715430736512143\n",
      "Epoch 843, Loss: 0.00012629595306500589\n",
      "Epoch 844, Loss: 0.00012544339307984904\n",
      "Epoch 845, Loss: 0.0001245965882951465\n",
      "Epoch 846, Loss: 0.0001237554998604732\n",
      "Epoch 847, Loss: 0.0001229200891876584\n",
      "Epoch 848, Loss: 0.00012209031794898067\n",
      "Epoch 849, Loss: 0.0001212661480754929\n",
      "Epoch 850, Loss: 0.00012044754175522179\n",
      "Epoch 851, Loss: 0.00011963446143142334\n",
      "Epoch 852, Loss: 0.00011882686980090691\n",
      "Epoch 853, Loss: 0.00011802472981228381\n",
      "Epoch 854, Loss: 0.00011722800466427356\n",
      "Epoch 855, Loss: 0.0001164366578040461\n",
      "Epoch 856, Loss: 0.00011565065292549501\n",
      "Epoch 857, Loss: 0.00011486995396762235\n",
      "Epoch 858, Loss: 0.00011409452511282193\n",
      "Epoch 859, Loss: 0.0001133243307853235\n",
      "Epoch 860, Loss: 0.00011255933564947176\n",
      "Epoch 861, Loss: 0.0001117995046081444\n",
      "Epoch 862, Loss: 0.00011104480280117804\n",
      "Epoch 863, Loss: 0.00011029519560372156\n",
      "Epoch 864, Loss: 0.00010955064862461892\n",
      "Epoch 865, Loss: 0.00010881112770489816\n",
      "Epoch 866, Loss: 0.00010807659891620141\n",
      "Epoch 867, Loss: 0.00010734702855918475\n",
      "Epoch 868, Loss: 0.00010662238316196118\n",
      "Epoch 869, Loss: 0.000105902629478639\n",
      "Epoch 870, Loss: 0.0001051877344877472\n",
      "Epoch 871, Loss: 0.0001044776653906871\n",
      "Epoch 872, Loss: 0.00010377238961031535\n",
      "Epoch 873, Loss: 0.00010307187478937434\n",
      "Epoch 874, Loss: 0.00010237608878904316\n",
      "Epoch 875, Loss: 0.00010168499968744224\n",
      "Epoch 876, Loss: 0.00010099857577819957\n",
      "Epoch 877, Loss: 0.00010031678556894897\n",
      "Epoch 878, Loss: 9.963959777994356e-05\n",
      "Epoch 879, Loss: 9.896698134257175e-05\n",
      "Epoch 880, Loss: 9.829890539794954e-05\n",
      "Epoch 881, Loss: 9.763533929553004e-05\n",
      "Epoch 882, Loss: 9.697625259162664e-05\n",
      "Epoch 883, Loss: 9.632161504810885e-05\n",
      "Epoch 884, Loss: 9.567139663094005e-05\n",
      "Epoch 885, Loss: 9.502556750884372e-05\n",
      "Epoch 886, Loss: 9.43840980519124e-05\n",
      "Epoch 887, Loss: 9.374695883024566e-05\n",
      "Epoch 888, Loss: 9.311412061263569e-05\n",
      "Epoch 889, Loss: 9.248555436516782e-05\n",
      "Epoch 890, Loss: 9.186123124994543e-05\n",
      "Epoch 891, Loss: 9.124112262373569e-05\n",
      "Epoch 892, Loss: 9.062520003664147e-05\n",
      "Epoch 893, Loss: 9.001343523085363e-05\n",
      "Epoch 894, Loss: 8.940580013928982e-05\n",
      "Epoch 895, Loss: 8.880226688433532e-05\n",
      "Epoch 896, Loss: 8.820280777657589e-05\n",
      "Epoch 897, Loss: 8.760739531351341e-05\n",
      "Epoch 898, Loss: 8.701600217829641e-05\n",
      "Epoch 899, Loss: 8.642860123847193e-05\n",
      "Epoch 900, Loss: 8.584516554476236e-05\n",
      "Epoch 901, Loss: 8.526566832978056e-05\n",
      "Epoch 902, Loss: 8.469008300688369e-05\n",
      "Epoch 903, Loss: 8.411838316884334e-05\n",
      "Epoch 904, Loss: 8.355054258673532e-05\n",
      "Epoch 905, Loss: 8.298653520867646e-05\n",
      "Epoch 906, Loss: 8.242633515863144e-05\n",
      "Epoch 907, Loss: 8.186991673528829e-05\n",
      "Epoch 908, Loss: 8.131725441078044e-05\n",
      "Epoch 909, Loss: 8.07683228295821e-05\n",
      "Epoch 910, Loss: 8.022309680735412e-05\n",
      "Epoch 911, Loss: 7.968155132973116e-05\n",
      "Epoch 912, Loss: 7.914366155123415e-05\n",
      "Epoch 913, Loss: 7.860940279407792e-05\n",
      "Epoch 914, Loss: 7.807875054708274e-05\n",
      "Epoch 915, Loss: 7.755168046453211e-05\n",
      "Epoch 916, Loss: 7.702816836504308e-05\n",
      "Epoch 917, Loss: 7.650819023047535e-05\n",
      "Epoch 918, Loss: 7.59917222048204e-05\n",
      "Epoch 919, Loss: 7.54787405931056e-05\n",
      "Epoch 920, Loss: 7.496922186033111e-05\n",
      "Epoch 921, Loss: 7.44631426303504e-05\n",
      "Epoch 922, Loss: 7.396047968481563e-05\n",
      "Epoch 923, Loss: 7.346120996213446e-05\n",
      "Epoch 924, Loss: 7.29653105563685e-05\n",
      "Epoch 925, Loss: 7.247275871622154e-05\n",
      "Epoch 926, Loss: 7.198353184397137e-05\n",
      "Epoch 927, Loss: 7.149760749444612e-05\n",
      "Epoch 928, Loss: 7.101496337398002e-05\n",
      "Epoch 929, Loss: 7.053557733942517e-05\n",
      "Epoch 930, Loss: 7.005942739707894e-05\n",
      "Epoch 931, Loss: 6.958649170173053e-05\n",
      "Epoch 932, Loss: 6.911674855562855e-05\n",
      "Epoch 933, Loss: 6.865017640748637e-05\n",
      "Epoch 934, Loss: 6.818675385150284e-05\n",
      "Epoch 935, Loss: 6.772645962638222e-05\n",
      "Epoch 936, Loss: 6.726927261434532e-05\n",
      "Epoch 937, Loss: 6.681517184016713e-05\n",
      "Epoch 938, Loss: 6.6364136470224e-05\n",
      "Epoch 939, Loss: 6.591614581152477e-05\n",
      "Epoch 940, Loss: 6.547117931077236e-05\n",
      "Epoch 941, Loss: 6.502921655339863e-05\n",
      "Epoch 942, Loss: 6.459023726265682e-05\n",
      "Epoch 943, Loss: 6.415422129866632e-05\n",
      "Epoch 944, Loss: 6.372114865753967e-05\n",
      "Epoch 945, Loss: 6.329099947035705e-05\n",
      "Epoch 946, Loss: 6.286375400239567e-05\n",
      "Epoch 947, Loss: 6.243939265210032e-05\n",
      "Epoch 948, Loss: 6.201789595025383e-05\n",
      "Epoch 949, Loss: 6.15992445590746e-05\n",
      "Epoch 950, Loss: 6.118341927130837e-05\n",
      "Epoch 951, Loss: 6.0770401009352205e-05\n",
      "Epoch 952, Loss: 6.0360170824410144e-05\n",
      "Epoch 953, Loss: 5.9952709895588255e-05\n",
      "Epoch 954, Loss: 5.9547999529032225e-05\n",
      "Epoch 955, Loss: 5.914602115709983e-05\n",
      "Epoch 956, Loss: 5.8746756337471605e-05\n",
      "Epoch 957, Loss: 5.835018675233773e-05\n",
      "Epoch 958, Loss: 5.7956294207528727e-05\n",
      "Epoch 959, Loss: 5.756506063169906e-05\n",
      "Epoch 960, Loss: 5.717646807551054e-05\n",
      "Epoch 961, Loss: 5.6790498710762455e-05\n",
      "Epoch 962, Loss: 5.640713482962759e-05\n",
      "Epoch 963, Loss: 5.6026358843810856e-05\n",
      "Epoch 964, Loss: 5.5648153283735375e-05\n",
      "Epoch 965, Loss: 5.527250079775387e-05\n",
      "Epoch 966, Loss: 5.489938415135972e-05\n",
      "Epoch 967, Loss: 5.452878622639099e-05\n",
      "Epoch 968, Loss: 5.4160690020235364e-05\n",
      "Epoch 969, Loss: 5.37950786450523e-05\n",
      "Epoch 970, Loss: 5.3431935327009146e-05\n",
      "Epoch 971, Loss: 5.307124340549682e-05\n",
      "Epoch 972, Loss: 5.2712986332395735e-05\n",
      "Epoch 973, Loss: 5.2357147671258936e-05\n",
      "Epoch 974, Loss: 5.200371109661822e-05\n",
      "Epoch 975, Loss: 5.165266039320952e-05\n",
      "Epoch 976, Loss: 5.1303979455222446e-05\n",
      "Epoch 977, Loss: 5.0957652285573415e-05\n",
      "Epoch 978, Loss: 5.061366299516187e-05\n",
      "Epoch 979, Loss: 5.027199580215707e-05\n",
      "Epoch 980, Loss: 4.993263503124045e-05\n",
      "Epoch 981, Loss: 4.9595565112934016e-05\n",
      "Epoch 982, Loss: 4.9260770582859775e-05\n",
      "Epoch 983, Loss: 4.892823608101715e-05\n",
      "Epoch 984, Loss: 4.8597946351103834e-05\n",
      "Epoch 985, Loss: 4.826988623979296e-05\n",
      "Epoch 986, Loss: 4.79440406960687e-05\n",
      "Epoch 987, Loss: 4.762039477051208e-05\n",
      "Epoch 988, Loss: 4.7298933614597504e-05\n",
      "Epoch 989, Loss: 4.6979642480068914e-05\n",
      "Epoch 990, Loss: 4.666250671820007e-05\n",
      "Epoch 991, Loss: 4.6347511779170714e-05\n",
      "Epoch 992, Loss: 4.603464321136423e-05\n",
      "Epoch 993, Loss: 4.572388666073511e-05\n",
      "Epoch 994, Loss: 4.541522787011381e-05\n",
      "Epoch 995, Loss: 4.5108652678592605e-05\n",
      "Epoch 996, Loss: 4.4804147020851046e-05\n",
      "Epoch 997, Loss: 4.450169692650926e-05\n",
      "Epoch 998, Loss: 4.420128851950476e-05\n",
      "Epoch 999, Loss: 4.3902908017438014e-05\n",
      "Epoch 1000, Loss: 4.360654173095005e-05\n",
      "[13.010210412839106]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Initialize weight and bias\n",
    "weight = random.random()\n",
    "bias = 0.0\n",
    "\n",
    "# Predict function\n",
    "def predict(x):\n",
    "    return [weight * xi + bias for xi in x]\n",
    "\n",
    "# Loss function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    error = [(yp - yt) ** 2 for yp, yt in zip(y_pred, y_true)]\n",
    "    return sum(error) / len(y_true)\n",
    "\n",
    "# Compute gradients\n",
    "def compute_gradients(x, y_true, y_pred):\n",
    "    d_weight = (-2 / len(x)) * sum(xi * (yt - yp) for xi, yt, yp in zip(x, y_true, y_pred))\n",
    "    d_bias = (-2 / len(x)) * sum(yt - yp for yt, yp in zip(y_true, y_pred))\n",
    "    return d_weight, d_bias\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(weight, bias, d_weight, d_bias, learning_rate):\n",
    "    weight -= learning_rate * d_weight\n",
    "    bias -= learning_rate * d_bias\n",
    "    return weight, bias\n",
    "\n",
    "# Train function\n",
    "def train(x, y, epochs, learning_rate):\n",
    "    global weight, bias\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(x)\n",
    "        loss = mean_squared_error(y, y_pred)\n",
    "        d_weight, d_bias = compute_gradients(x, y, y_pred)\n",
    "        weight, bias = update_parameters(weight, bias, d_weight, d_bias, learning_rate)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "\n",
    "# Sample data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [3, 5, 7, 9, 11]\n",
    "\n",
    "# Train the model\n",
    "train(x, y, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Test prediction\n",
    "print(predict([6]))  # Should be close to 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da40b6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.527009826075158\n",
      "Epoch 100: Loss = 0.014755881464826027\n",
      "Epoch 200: Loss = 0.010354525312675162\n",
      "Epoch 300: Loss = 0.010071905817989064\n",
      "Epoch 400: Loss = 0.010053758279817753\n",
      "Epoch 500: Loss = 0.01005259299169054\n",
      "Epoch 600: Loss = 0.010052518166311666\n",
      "Epoch 700: Loss = 0.010052513361631009\n",
      "Epoch 800: Loss = 0.010052513053113271\n",
      "Epoch 900: Loss = 0.01005251303330276\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weight: 2.04616794688804\n",
      "Final bias: 0.984206619932021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Generating a simple dataset\n",
    "def generate_data(num_samples=100):\n",
    "    x = [random.random() for _ in range(num_samples)]  # Generate random x values\n",
    "    y = [2 * xi + 1 + random.gauss(0, 0.1) for xi in x]  # Generate y = 2x + 1 + noise\n",
    "    return x, y\n",
    "\n",
    "# 2. Initialize parameters\n",
    "w = random.random()  # Initialize weight randomly\n",
    "b = random.random()  # Initialize bias randomly\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Generate the dataset\n",
    "x, y = generate_data()\n",
    "\n",
    "# Number of samples\n",
    "m = len(y)\n",
    "\n",
    "# 3. Gradient Descent\n",
    "for epoch in range(epochs):\n",
    "    # 4. Compute predictions\n",
    "    y_pred = [w * xi + b for xi in x]\n",
    "    \n",
    "    # 5. Compute the loss (Mean Squared Error)\n",
    "    loss = sum([(y_pred[i] - y[i]) ** 2 for i in range(m)]) / m\n",
    "    \n",
    "    # 6. Calculate gradients\n",
    "    dw = (2/m) * sum([(y_pred[i] - y[i]) * x[i] for i in range(m)])\n",
    "    db = (2/m) * sum([(y_pred[i] - y[i]) for i in range(m)])\n",
    "    \n",
    "    # 7. Update parameters\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "# 8. Plotting the result\n",
    "plt.scatter(x, y, label='True data')\n",
    "plt.plot(x, [w * xi + b for xi in x], color='red', label='Model prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final weight: {w}\")\n",
    "print(f\"Final bias: {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
