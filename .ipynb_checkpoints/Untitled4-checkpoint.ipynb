{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7760fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (m): 0.42155649706923126\n",
      "Intercept (b): 14.245236399085083\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = [5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]\n",
    "Y = [17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]\n",
    "\n",
    "# Number of data points\n",
    "N = len(X)\n",
    "\n",
    "# Calculate sums\n",
    "sum_x = sum(X)\n",
    "sum_y = sum(Y)\n",
    "sum_xy = sum(x * y for x, y in zip(X, Y))\n",
    "sum_x2 = sum(x ** 2 for x in X)\n",
    "\n",
    "# Calculate slope (m) and intercept (b)\n",
    "m = (N * sum_xy - sum_x * sum_y) / (N * sum_x2 - sum_x ** 2)\n",
    "b = (sum_y * sum_x2 - sum_x * sum_xy) / (N * sum_x2 - sum_x ** 2)\n",
    "\n",
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Intercept (b): {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4567ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9183016607031469\n",
      "Epoch 100, Loss: 0.891191719157456\n",
      "Epoch 200, Loss: 0.8907149109530661\n",
      "Epoch 300, Loss: 0.8907065248758261\n",
      "Epoch 400, Loss: 0.8907063773819474\n",
      "Epoch 500, Loss: 0.8907063747878331\n",
      "Epoch 600, Loss: 0.8907063747422079\n",
      "Epoch 700, Loss: 0.8907063747414053\n",
      "Epoch 800, Loss: 0.8907063747413912\n",
      "Epoch 900, Loss: 0.8907063747413911\n",
      "Weight W: [[0.33059586]]\n",
      "Bias b: [[-1.020517e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]).reshape(-1, 1)\n",
    "Y = np.array([17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "Y = (Y - np.mean(Y)) / np.std(Y)\n",
    "\n",
    "# Neural Network parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(input_size, output_size)\n",
    "b = np.zeros((1, output_size))\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X):\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "# Mean squared error loss\n",
    "def compute_loss(Y, Y_pred):\n",
    "    return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "# Backward propagation\n",
    "def backward(X, Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    dW = -2 * np.dot(X.T, (Y - Y_pred)) / m\n",
    "    db = -2 * np.sum(Y - Y_pred) / m\n",
    "    return dW, db\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Y_pred = forward(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(Y, Y_pred)\n",
    "    \n",
    "    # Backward pass\n",
    "    dW, db = backward(X, Y, Y_pred)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Results\n",
    "print(\"Weight W:\", W)\n",
    "print(\"Bias b:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af486bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 93.06719284724281\n",
      "Epoch 100, Loss: 0.77477924891854\n",
      "Epoch 200, Loss: 0.6986201469078495\n",
      "Epoch 300, Loss: 0.61567355879671\n",
      "Epoch 400, Loss: 0.5310112213929565\n",
      "Epoch 500, Loss: 0.44948443303136953\n",
      "Epoch 600, Loss: 0.3856491845664447\n",
      "Epoch 700, Loss: 0.3584258472760138\n",
      "Epoch 800, Loss: 0.34761536613110333\n",
      "Epoch 900, Loss: 0.3447859089732293\n",
      "Weight W1: [[ 0.70812921 -0.2477796  -0.02124272  1.5482106  -0.18764849 -0.22809432\n",
      "   1.12648154  0.79368996 -0.57397788  0.76859532]]\n",
      "Bias b1: [[-0.12553374  0.16548536 -0.54955813 -0.0202135   0.08582663  0.17980407\n",
      "  -0.37969619 -0.09720338  0.09703399  0.39425476]]\n",
      "Weight W2: [[-0.47364018 -0.60015013  0.2627328  -1.91328024 -1.72491783 -0.56227625\n",
      "  -1.01300105  0.36537861 -0.93083486 -1.4123037 ]\n",
      " [ 1.46452771 -0.22931201  0.06752811 -1.42474819 -0.54573858  0.09044887\n",
      "  -1.17883973  0.40774098 -0.58431773 -0.29169375]\n",
      " [-0.61503619  1.67700124  0.00700571 -1.05771093  0.82254491 -1.22084365\n",
      "   0.20813427 -1.9003801  -1.32818605  0.19686124]\n",
      " [ 0.70712229 -0.24079281 -0.05447873 -0.3011037  -1.47852199 -0.71992312\n",
      "  -0.45511411  1.20650355  0.28427344 -1.76304016]\n",
      " [ 0.32444733 -0.38374743 -0.676922    0.61167629  1.02343851  0.91678736\n",
      "  -0.85897808 -0.28607841  0.34368767  0.97554513]\n",
      " [-0.48028864 -0.18878335 -1.10668487 -1.19620662  0.80870895  1.33300366\n",
      "  -0.10390384  1.04031372  0.37931009 -0.64511975]\n",
      " [ 0.32889506  1.11067129  0.11539711  1.56464366 -2.6197451   0.8219025\n",
      "  -0.01630814 -0.16450646  0.0652711  -1.98756891]\n",
      " [-0.23546586  0.14943002  1.52875227 -0.51827022 -0.8084936  -0.50176706\n",
      "   0.89547863  0.40038957 -0.55540507  0.51326743]\n",
      " [ 0.10148567  0.97387584 -0.70205309 -0.32766215 -0.41304938 -1.49161514\n",
      "   0.25564227  0.3078369   0.03184585 -0.23458713]\n",
      " [-1.43228698 -0.56747248 -0.38101969 -0.80227727 -0.14237644  0.39032378\n",
      "   1.94235535  0.25967056  0.21836394 -0.07444592]]\n",
      "Bias b2: [[-0.07463316 -0.31453986 -0.20723261  0.          0.05326938 -0.14122967\n",
      "   0.04249393  0.34224678  0.06580238  0.        ]]\n",
      "Weight W3: [[-1.92243221 -0.02651388  0.04517451  2.46470665 -0.19028452  0.30829345\n",
      "  -0.04816669 -1.16867321  1.13837916  0.75193303]\n",
      " [ 0.71532675 -0.90938745  1.00163226 -1.41025438  0.64129028  2.01508304\n",
      "  -0.99247173 -0.56629773 -0.01875061 -0.50347565]\n",
      " [-1.57087304  0.06856297 -1.15186727  0.42207983 -0.90714294  1.54870823\n",
      "  -0.80927159 -0.38472866  0.78708276 -1.23385212]\n",
      " [ 0.22745993  1.30714275 -1.60748323  0.18463386  0.25988279  0.78182287\n",
      "  -1.23695071 -1.32045661  0.52194157  0.29698467]\n",
      " [ 0.26430066  0.34644821 -0.68002472  0.21927548  0.29350076 -0.70037745\n",
      "   1.87186669  0.46022667 -1.1913035   0.65655361]\n",
      " [-0.97443228  0.7870846   1.15859558 -0.82079607  0.96339715  0.41306702\n",
      "   0.82232573  1.89631268 -0.24538812 -0.75370843]\n",
      " [-0.92099872 -0.81581028 -0.16309078  0.43558634  0.28628364  0.72093312\n",
      "   0.05754399  1.55442728 -0.29003629  2.71037351]\n",
      " [ 0.68279792 -0.85715756 -1.11114658  0.47322854 -0.21898379  0.72586605\n",
      "   0.57236646 -0.08496774 -0.85867461 -1.53131021]\n",
      " [-0.42984608  0.85639879  0.21409374 -1.26347583  0.17346127  0.39852839\n",
      "  -0.872176    0.13599636  0.05820872 -1.14328844]\n",
      " [ 0.35778736  0.56078453  1.08305124  1.05380205 -1.37766937 -0.93782504\n",
      "   0.51503527  0.51378595  0.51504769  3.85273149]]\n",
      "Bias b3: [[ 0.16629528  0.         -0.0889756  -0.05184322  0.00845018 -0.00042841\n",
      "   0.24275021 -0.02011887 -0.02626093 -0.01132564]]\n",
      "Weight W4: [[ 0.67764307  1.06567392  0.93632064  0.62963969 -0.398309    0.75813235\n",
      "  -0.80973114 -0.23681861 -0.50677368  0.26923829]\n",
      " [ 2.31465857 -1.86726519  0.68626019 -1.61271587 -0.47193187  1.0889506\n",
      "   0.06428002 -1.07774478 -0.71530371  0.67959775]\n",
      " [-0.73036663 -0.081419    0.04557184 -0.65160035  1.79002975  0.63391902\n",
      "  -2.21001953  0.18645431 -0.66178646  0.88771502]\n",
      " [-0.78777397 -0.11473644  0.49919909  0.84567352 -1.20029641 -0.33479337\n",
      "  -0.47981458 -0.65332923  1.80322131  0.40311989]\n",
      " [-1.26088283  0.80051084  2.1221562   1.0324619  -1.65879717 -0.48423407\n",
      "   1.19367675 -0.70766947  0.44319828  0.78892956]\n",
      " [-0.85985379 -0.85758347 -3.25446759 -1.06161905 -1.200757   -1.24845053\n",
      "   1.13397404 -1.43014138 -0.42791702  0.33322971]\n",
      " [ 1.54435033 -1.43586215  1.14156554 -0.01790446 -0.98150865  0.46128733\n",
      "   0.19550779 -0.60021688  0.13382339 -0.30670536]\n",
      " [ 0.11506297  0.66213067  1.58555624 -1.23816517  2.13303337 -1.95212541\n",
      "  -0.16668123  0.58831721  0.40008586 -0.79500128]\n",
      " [-0.20812225 -0.54506836 -0.58936476  0.8496021   0.29515314 -0.6929096\n",
      "   0.86728437  0.30729952  0.81286212  0.63579589]\n",
      " [-0.82899501 -0.56018104  0.74729361  0.61037027 -0.02090159  0.11732738\n",
      "   1.2772036  -0.59157139  0.54924917 -0.21152983]]\n",
      "Bias b4: [[ 0.29095715 -0.06328923 -0.05240079 -0.08766036 -0.0751952  -0.00566116\n",
      "  -0.04702668  0.          0.07793577  0.35952627]]\n",
      "Weight W5: [[-0.66797112]\n",
      " [ 0.93643003]\n",
      " [ 0.82417092]\n",
      " [ 0.82815269]\n",
      " [ 1.07310837]\n",
      " [-0.02147045]\n",
      " [-0.06897526]\n",
      " [-0.31026676]\n",
      " [ 0.59389459]\n",
      " [-0.88130217]]\n",
      "Bias b5: [[-0.64808241]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]).reshape(-1, 1)\n",
    "Y = np.array([17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]).reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "Y = (Y - np.mean(Y)) / np.std(Y)\n",
    "\n",
    "# Neural Network parameters\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, hidden_size)\n",
    "b2 = np.zeros((1, hidden_size))\n",
    "W3 = np.random.randn(hidden_size, hidden_size)\n",
    "b3 = np.zeros((1, hidden_size))\n",
    "W4 = np.random.randn(hidden_size, hidden_size)\n",
    "b4 = np.zeros((1, hidden_size))\n",
    "W5 = np.random.randn(hidden_size, output_size)\n",
    "b5 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# Forward propagation\n",
    "def forward(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = relu(Z3)\n",
    "    Z4 = np.dot(A3, W4) + b4\n",
    "    A4 = relu(Z4)\n",
    "    Z5 = np.dot(A4, W5) + b5\n",
    "    return Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5\n",
    "\n",
    "# Mean squared error loss\n",
    "def compute_loss(Y, Y_pred):\n",
    "    return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "# Backward propagation\n",
    "def backward(X, Y, Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5):\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    dZ5 = Z5 - Y\n",
    "    dW5 = np.dot(A4.T, dZ5) / m\n",
    "    db5 = np.sum(dZ5, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA4 = np.dot(dZ5, W5.T)\n",
    "    dZ4 = dA4 * relu_derivative(Z4)\n",
    "    dW4 = np.dot(A3.T, dZ4) / m\n",
    "    db4 = np.sum(dZ4, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA3 = np.dot(dZ4, W4.T)\n",
    "    dZ3 = dA3 * relu_derivative(Z3)\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA2 = np.dot(dZ3, W3.T)\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5 = forward(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(Y, Z5)\n",
    "    \n",
    "    # Backward pass\n",
    "    dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5 = backward(X, Y, Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    W4 -= learning_rate * dW4\n",
    "    b4 -= learning_rate * db4\n",
    "    W5 -= learning_rate * dW5\n",
    "    b5 -= learning_rate * db5\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Results\n",
    "print(\"Weight W1:\", W1)\n",
    "print(\"Bias b1:\", b1)\n",
    "print(\"Weight W2:\", W2)\n",
    "print(\"Bias b2:\", b2)\n",
    "print(\"Weight W3:\", W3)\n",
    "print(\"Bias b3:\", b3)\n",
    "print(\"Weight W4:\", W4)\n",
    "print(\"Bias b4:\", b4)\n",
    "print(\"Weight W5:\", W5)\n",
    "print(\"Bias b5:\", b5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab57f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py): started\n",
      "  Building wheel for fpdf (setup.py): finished with status 'done'\n",
      "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40713 sha256=87fdac3482530be11233fb2f88bf90913036bc4d09424c33422269b4b92e82dc\n",
      "  Stored in directory: c:\\users\\ratho\\appdata\\local\\pip\\cache\\wheels\\65\\4f\\66\\bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    }
   ],
   "source": [
    "pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = [5.40, 6.18, 8.71, 6.28, 7.60, 8.49, 5.74, 9.93, 8.38, 6.94]\n",
    "Y = [17, 18, 21, 17, 17, 16.5, 17, 17.5, 19, 13.5]\n",
    "\n",
    "# Number of data points\n",
    "N = len(X)\n",
    "\n",
    "# Calculate sums\n",
    "sum_x = sum(X)\n",
    "sum_y = sum(Y)\n",
    "sum_xy = sum(x * y for x, y in zip(X, Y))\n",
    "sum_x2 = sum(x ** 2 for x in X)\n",
    "\n",
    "# Calculate slope (m) and intercept (b)\n",
    "m = (N * sum_xy - sum_x * sum_y) / (N * sum_x2 - sum_x ** 2)\n",
    "b = (sum_y * sum_x2 - sum_x * sum_xy) / (N * sum_x2 - sum_x ** 2)\n",
    "\n",
    "# Print all necessary steps\n",
    "print(f\"Number of data points (N): {N}\")\n",
    "print(f\"Sum of X (sum_x): {sum_x}\")\n",
    "print(f\"Sum of Y (sum_y): {sum_y}\")\n",
    "print(f\"Sum of X*Y (sum_xy): {sum_xy}\")\n",
    "print(f\"Sum of X^2 (sum_x2): {sum_x2}\")\n",
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Intercept (b): {b}\")\n",
    "\n",
    "# Directly convert to PDF (This part of the code can be adapted to use a library like FPDF in Python)\n",
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Linear Regression Calculation', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(10)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.chapter_title('Calculation Steps and Results')\n",
    "\n",
    "body = (\n",
    "    f\"Number of data points (N): {N}\\n\"\n",
    "    f\"Sum of X (sum_x): {sum_x}\\n\"\n",
    "    f\"Sum of Y (sum_y): {sum_y}\\n\"\n",
    "    f\"Sum of X*Y (sum_xy): {sum_xy}\\n\"\n",
    "    f\"Sum of X^2 (sum_x2): {sum_x2}\\n\"\n",
    "    f\"Slope (m): {m}\\n\"\n",
    "    f\"Intercept (b): {b}\\n\"\n",
    ")\n",
    "\n",
    "pdf.chapter_body(body)\n",
    "\n",
    "pdf.output('linear_regression_calculation.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
